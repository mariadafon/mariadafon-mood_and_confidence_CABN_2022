{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats,signal\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "from IPython.display import HTML, display, Image\n",
    "import tabulate\n",
    "import math as m\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 18\n",
    "mpl.rcParams['lines.markersize'] = 10\n",
    "mpl.rcParams['xtick.labelsize'] = 20\n",
    "mpl.rcParams['ytick.labelsize'] = 20\n",
    "mpl.rcParams['axes.linewidth'] = 1\n",
    "#mpl.rcParams['xtick.major.size'] = 20\n",
    "mpl.rcParams['xtick.major.width'] = 1\n",
    "#mpl.rcParams['xtick.minor.size'] = 10\n",
    "mpl.rcParams['xtick.minor.width'] = 1\n",
    "mpl.rcParams['ytick.major.width'] = 1\n",
    "mpl.rcParams['ytick.minor.width'] = 1\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "fday = [1,2,3,4,5,6,7,8,9,10]\n",
    "fsession = [1,2]\n",
    "excluded = [(3062,1),(3062,2),(3062,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.path.abspath(os.getcwd())\n",
    "parent_path = os.path.abspath(os.path.join(current_path, os.pardir))\n",
    "grand_parent_path = os.path.abspath(os.path.join(parent_path, os.pardir))\n",
    "main_path = os.path.abspath(os.path.join(grand_parent_path, os.pardir))\n",
    "\n",
    "path_results = main_path+'/results/dots/'\n",
    "path_data = main_path+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeWithZero(domain, xlista, ylista):\n",
    "    if len(domain)==len(xlista):\n",
    "        return ylista\n",
    "    elif len(domain)<len(xlista):\n",
    "        print(domain,xlista)\n",
    "        return 'Error'\n",
    "    else:\n",
    "        set1 = set(domain)\n",
    "        set2 = set(xlista)\n",
    "        # missed values\n",
    "        missing = list(sorted(set1 - set2))\n",
    "        # index of missed values\n",
    "        index = []\n",
    "        [index.append(domain.index(elem)) for elem in missing]\n",
    "        # insert zero in ylista\n",
    "        [ylista.insert(elem,0) for elem in index]\n",
    "        return ylista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of participants \n",
    "participants = [3051+i for i in range(28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "accQ,accF,accS,DaccNO,SaccNO,accNO,globalDaccNO,globalSaccNO = [],[],[],[],[],[],[],[] # list of arrays of acc per RT quartile for every subject and session\n",
    "for day in fday:\n",
    "    for session in fsession:\n",
    "        sessionid = 2*day-2+session\n",
    "        \n",
    "        path = path_data+'jatos_dots_data/tanda1/day'+str(day)+'/session'+str(session)+'/'\n",
    "        data_files = [f for f in os.listdir(path) if f.endswith('_day'+str(day)+'_session'+str(session))]\n",
    "        \n",
    "        # sort files\n",
    "        subj_data = [int(re.search('%s(.*)%s' % ('', '_day'), f).group(1)) for f in data_files]\n",
    "        sorted_subj_data = sorted(subj_data)\n",
    "        index_subj_data = [subj_data.index(elem) for elem in sorted_subj_data]\n",
    "        sorted_data_files = [data_files[i] for i in index_subj_data]\n",
    "\n",
    "        Ddf,Sdf = {},{}\n",
    "        for name in sorted_subj_data: \n",
    "            Ddf[name] = pd.DataFrame()\n",
    "            Sdf[name] = pd.DataFrame()\n",
    "                       \n",
    "        ind = -1\n",
    "        for ses in sorted_data_files:\n",
    "            ind += 1\n",
    "            data = [] \n",
    "            for line in open(path+ses, 'r'):\n",
    "                if line.strip():\n",
    "                    data.append(json.loads(line))\n",
    "\n",
    "            if day==1 and session==1:\n",
    "                if len(data)==5:\n",
    "                    deterministic = data[3]\n",
    "                    stochastic = data[4]\n",
    "                else:\n",
    "                    deterministic = data[len(data)-2]\n",
    "                    stochastic = data[len(data)-1]\n",
    "            else:\n",
    "                if len(data)==4:\n",
    "                    deterministic = data[2]\n",
    "                    stochastic = data[3]\n",
    "                elif len(data)>4:\n",
    "                    deterministic = data[len(data)-2]\n",
    "                    stochastic = data[len(data)-1]\n",
    "                    print('participant '+ str(sorted_subj_data[ind])+' has repeated practice '+str(len(data)-3))\n",
    "                    pract = []\n",
    "                    for j in range(len(data)):\n",
    "                        if len(data[j])==10:\n",
    "                            pract.append(data[j])\n",
    "                else: \n",
    "                    for dd in range(len(data)):\n",
    "                        print(len(data[dd]))\n",
    "                    deterministic = data[2]\n",
    "                    print('participant '+ str(sorted_subj_data[ind])+' missed stochastic')       \n",
    "            Ddf[sorted_subj_data[ind]] = pd.DataFrame.from_dict(deterministic)\n",
    "            Sdf[sorted_subj_data[ind]] = pd.DataFrame.from_dict(stochastic)\n",
    "\n",
    "        for part in sorted_subj_data:\n",
    "            if (part,sessionid) not in excluded:\n",
    "                ## deterministic dataframe\n",
    "                # discrimination RT\n",
    "                discrimination_RT = np.array(list(Ddf[part].discrimination_t_keydown))-np.array(list(Ddf[part].t_offset))\n",
    "                Ddf[part][\"discrimination_RT\"] = discrimination_RT \n",
    "                Ddf[part]['signal'] = np.abs(Ddf[part]['dots_num_left']-Ddf[part]['dots_num_right'])\n",
    "                Dsignal = np.array(np.abs(Ddf[part]['dots_num_left']-Ddf[part]['dots_num_right']))/100\n",
    "                Dorientation = Ddf[part]['orientation']\n",
    "                signedDsignal = []\n",
    "                ind = -1\n",
    "                for elem in Dorientation:\n",
    "                    ind += 1\n",
    "                    if elem:\n",
    "                        signedDsignal.append(Dsignal[ind])\n",
    "                    else: \n",
    "                        signedDsignal.append(-Dsignal[ind])  \n",
    "                # signed signal (neg signal to the left and pos signal tothe right)\n",
    "                Ddf[part]['signed_signal'] = signedDsignal\n",
    "                Dcorrect_list = list(Ddf[part]['discrimination_is_correct'])\n",
    "                Doptout_list = list(Ddf[part][\"optout\"])\n",
    "                Dbool_correct_list = [elem==1 for elem in Dcorrect_list]\n",
    "                Dbool_orientation_list = [elem==1 for elem in Dorientation]\n",
    "                Dxor = np.logical_xor(Dbool_correct_list,np.logical_not(Dbool_orientation_list))\n",
    "                Dresp = []\n",
    "                ind = -1\n",
    "                for elem in Doptout_list:\n",
    "                    ind += 1\n",
    "                    if elem==False and Dxor[ind]==True:\n",
    "                        Dresp.append(1)\n",
    "                    elif elem==False and Dxor[ind]==False:\n",
    "                        Dresp.append(0)\n",
    "                    elif elem==True: \n",
    "                        Dresp.append(2)\n",
    "                # 1 for rightward answers, 0 for left\n",
    "                Ddf[part]['resp_is_R'] = [elem==1 for elem in Dresp]\n",
    "                Ddf[part]['resp_is_R'] = Ddf[part]['resp_is_R'].astype(int)\n",
    "                # 1 for rightward answers, 0 for left, and 2 for optout\n",
    "                Ddf[part]['answer'] = Dresp\n",
    "\n",
    "                ## stochastic dataframe\n",
    "                # discrimination RT\n",
    "                Sdiscrimination_RT = np.array(list(Sdf[part].discrimination_t_keydown))-np.array(list(Sdf[part].t_offset))\n",
    "                Sdf[part][\"discrimination_RT\"] = Sdiscrimination_RT \n",
    "                Sdf[part]['signal'] = np.abs(Sdf[part]['dots_num_left']-Sdf[part]['dots_num_right'])\n",
    "                Ssignal = np.array(np.abs(Sdf[part]['dots_num_left']-Sdf[part]['dots_num_right']))/100\n",
    "                Sorientation = Sdf[part]['orientation']\n",
    "                signedSsignal = []\n",
    "                ind = -1\n",
    "                for elem in Sorientation:\n",
    "                    ind += 1\n",
    "                    if elem:\n",
    "                        signedSsignal.append(Ssignal[ind])\n",
    "                    else: \n",
    "                        signedSsignal.append(-Ssignal[ind])  \n",
    "                # signed signal (neg signal to the left and pos signal tothe right)\n",
    "                Sdf[part]['signed_signal'] = signedSsignal\n",
    "                Scorrect_list = list(Sdf[part]['discrimination_is_correct'])\n",
    "                Soptout_list = list(Sdf[part][\"optout\"])\n",
    "                Sbool_correct_list = [elem==1 for elem in Scorrect_list]\n",
    "                Sbool_orientation_list = [elem==1 for elem in Sorientation]\n",
    "                Sxor = np.logical_xor(Sbool_correct_list,np.logical_not(Sbool_orientation_list))\n",
    "                Sresp = []\n",
    "                ind = -1\n",
    "                for elem in Soptout_list:\n",
    "                    ind += 1\n",
    "                    if elem==False and Sxor[ind]==True:\n",
    "                        Sresp.append(1)\n",
    "                    elif elem==False and Sxor[ind]==False:\n",
    "                        Sresp.append(0)\n",
    "                    elif elem==True: \n",
    "                        Sresp.append(2)\n",
    "                # 1 for rightward answers, 0 for left\n",
    "                Sdf[part]['resp_is_R'] = [elem==1 for elem in Sresp]\n",
    "                Sdf[part]['resp_is_R'] = Sdf[part]['resp_is_R'].astype(int)\n",
    "                # 1 for rightward answers, 0 for left, and 2 for optout\n",
    "                Sdf[part]['answer'] = Sresp\n",
    "            \n",
    "        # deterministic non-optout & optout df\n",
    "        Ddf_no,Ddf_oo = {},{}\n",
    "        for name in sorted_subj_data: \n",
    "            Ddf_no[name] = pd.DataFrame()\n",
    "            Ddf_oo[name] = pd.DataFrame()\n",
    "        for part in sorted_subj_data:\n",
    "            Ddf_no[part] = Ddf[part][(Ddf[part]['focus']==0)]\n",
    "            Ddf_oo[part] = Ddf[part][(Ddf[part]['focus']==1)]\n",
    "                \n",
    "        # stochastic non-optout & optout df\n",
    "        Sdf_no,Sdf_oo = {},{}\n",
    "        for name in sorted_subj_data: \n",
    "            Sdf_no[name] = pd.DataFrame()\n",
    "            Sdf_oo[name] = pd.DataFrame()\n",
    "        for part in sorted_subj_data:\n",
    "            Sdf_no[part] = Sdf[part][(Sdf[part]['focus']==0)]\n",
    "            Sdf_oo[part] = Sdf[part][(Sdf[part]['focus']==1)]\n",
    "\n",
    "        # non-optout df for each participant\n",
    "        df_no = {}\n",
    "        for name in sorted_subj_data: \n",
    "            df_no[name] = pd.DataFrame()\n",
    "        for part in sorted_subj_data:\n",
    "            \n",
    "            if (part,sessionid) not in excluded:\n",
    "            \n",
    "                df_no[part] = pd.concat([Ddf_no[part],Sdf_no[part]])\n",
    "\n",
    "                df_no[part] = df_no[part].astype({'discrimination_is_correct': 'float'})\n",
    "                dfdiffNO = df_no[part].groupby(['difficulty'])[['discrimination_is_correct']].mean().reset_index()\n",
    "                dfdiffNO = dfdiffNO.sort_values('difficulty')\n",
    "                accNO.append(100*dfdiffNO['discrimination_is_correct'])\n",
    "                \n",
    "                Ddf_no[part] = Ddf_no[part].astype({'discrimination_is_correct': 'float'})\n",
    "                dfDdiffNO = Ddf_no[part].groupby(['difficulty'])[['discrimination_is_correct']].mean().reset_index()\n",
    "                dfDdiffNO = dfDdiffNO.sort_values('difficulty')\n",
    "                # accuracy in deterministic NO trials\n",
    "                DaccNO.append(100*dfDdiffNO['discrimination_is_correct'])\n",
    "                globalDaccNO.append(100*Ddf_no[part]['discrimination_is_correct'].mean())\n",
    "                \n",
    "                Sdf_no[part] = Sdf_no[part].astype({'discrimination_is_correct': 'float'})\n",
    "                dfSdiffNO = Sdf_no[part].groupby(['difficulty'])[['discrimination_is_correct']].mean().reset_index()\n",
    "                dfSdiffNO = dfSdiffNO.sort_values('difficulty')\n",
    "                # accuracy in stochastic NO trials\n",
    "                SaccNO.append(100*dfSdiffNO['discrimination_is_correct'])\n",
    "                globalSaccNO.append(100*Sdf_no[part]['discrimination_is_correct'].mean())\n",
    "\n",
    "                # medianRT\n",
    "                RT_list = list(df_no[part]['discrimination_t_keydown'])+list(Ddf_oo[part]['discrimination_t_keydown'])+\\\n",
    "                list(Sdf_oo[part]['discrimination_t_keydown'])\n",
    "                # global median RT for this participant\n",
    "                medianRT = np.nanmedian(RT_list)\n",
    "\n",
    "                # normalized RT \n",
    "                df_no[part]['norm_NO_RT'] = df_no[part]['discrimination_t_keydown']*(1./medianRT)\n",
    "                # sort rows by normalized NO RT\n",
    "                df_no[part] = df_no[part].sort_values('norm_NO_RT') \n",
    "                # index for each element\n",
    "                df_no[part]['idx'] = np.arange(df_no[part].shape[0])\n",
    "                # cast index in interval 0 - 100\n",
    "                df_no[part]['idx'] = df_no[part]['idx']/(df_no[part]['idx'].max()+1)*100 \n",
    "                # convert discrimination_is_correct and signed_signal to float\n",
    "                df_no[part] = df_no[part].astype({'discrimination_is_correct': 'float'})\n",
    "                df_no[part] = df_no[part].astype({'signed_signal': 'float'})\n",
    "                # columns to average\n",
    "                cols2mean = ['discrimination_is_correct','discrimination_t_keydown','norm_NO_RT']\n",
    "\n",
    "                # fast and slow dfs\n",
    "                dfS, dfF = df_no[part][(mask:=df_no[part]['idx'] >= 50)], df_no[part][~mask]\n",
    "\n",
    "                '''\n",
    "                # mean of every column group group by signed_signal\n",
    "                dfSsig = dfS.groupby(['signed_signal'])[cols2mean].mean().reset_index()\n",
    "                dfSsig = dfSsig.sort_values('signed_signal') \n",
    "                dfFsig = dfF.groupby(['signed_signal'])[cols2mean].mean().reset_index()\n",
    "                dfFsig = dfFsig.sort_values('signed_signal') \n",
    "                # mean accuracy per signed signal\n",
    "                accSmean = list(100*dfSsig['discrimination_is_correct'])\n",
    "                accFmean = list(100*dfFsig['discrimination_is_correct'])\n",
    "\n",
    "                # every values for signed signal\n",
    "                signed_signals = [-0.14, -0.1 , -0.06, -0.02,  0.02,  0.06,  0.1 ,  0.14]\n",
    "                # unique signed signal\n",
    "                Sunique = list(dfSsig['signed_signal'].unique())\n",
    "                Funique = list(dfFsig['signed_signal'].unique())\n",
    "\n",
    "                accSmean = completeWithZero(signed_signals, Sunique, accSmean)\n",
    "                accFmean = completeWithZero(signed_signals, Funique, accFmean)\n",
    "                '''\n",
    "                # mean of every column group group by difficulty\n",
    "                dfSsig = dfS.groupby(['difficulty'])[cols2mean].mean().reset_index()\n",
    "                dfSsig = dfSsig.sort_values('difficulty') \n",
    "                dfFsig = dfF.groupby(['difficulty'])[cols2mean].mean().reset_index()\n",
    "                dfFsig = dfFsig.sort_values('difficulty')             \n",
    "                # mean accuracy per signed signal\n",
    "                accSmean = list(100*dfSsig['discrimination_is_correct'])\n",
    "                accFmean = list(100*dfFsig['discrimination_is_correct'])\n",
    "\n",
    "                # every values for difficulty\n",
    "                diff_values = [1,2,3,4]\n",
    "                # unique difficulty\n",
    "                Sunique = list(dfSsig['difficulty'].unique())\n",
    "                Funique = list(dfFsig['difficulty'].unique())\n",
    "\n",
    "                accSmean = completeWithZero(diff_values, Sunique, accSmean)\n",
    "                accFmean = completeWithZero(diff_values, Funique, accFmean)\n",
    "\n",
    "                accS.append(accSmean)\n",
    "                accF.append(accFmean)\n",
    "\n",
    "                # define quartiles\n",
    "                df_no[part]['quartile'] = df_no[part]['idx'] // 25 \n",
    "                # mean of every column group by RT quartile \n",
    "                dfQ = df_no[part].groupby(['quartile'])[cols2mean].mean().reset_index()\n",
    "                # mean accuracy per quartile of RT\n",
    "                accQ.append(np.array(100*dfQ['discrimination_is_correct']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "for diff in np.arange(4):\n",
    "    ttest = stats.ttest_rel(np.array(DaccNO)[:,diff],np.array(SaccNO)[:,diff])\n",
    "    print(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(np.array(globalDaccNO),np.array(globalSaccNO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanACC_DNO = np.array(DaccNO).mean(axis=0) \n",
    "semACC_DNO = np.array(DaccNO).std(axis=0)/np.sqrt(len(DaccNO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanACC_SNO = np.array(SaccNO).mean(axis=0) \n",
    "semACC_SNO = np.array(SaccNO).std(axis=0)/np.sqrt(len(SaccNO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(4)+1,meanACC_DNO,color=[1,0,0.4])\n",
    "plt.fill_between(np.arange(4)+1,np.array(meanACC_DNO)-np.array(semACC_DNO), \n",
    "                                 np.array(meanACC_DNO)+np.array(semACC_DNO), alpha=0.2, color = [1,0,0.4])\n",
    "plt.plot(np.arange(4)+1,meanACC_SNO,color=[0.4,0,1])\n",
    "plt.fill_between(np.arange(4)+1,np.array(meanACC_SNO)-np.array(semACC_SNO), \n",
    "                                 np.array(meanACC_SNO)+np.array(semACC_SNO), alpha=0.2, color = [0.4,0,1])\n",
    "# plt.ylabel('accuracy (%)')\n",
    "plt.xlabel('difficulty')\n",
    "plt.legend(['DO','SO'],loc='best', shadow=True,fontsize=18)\n",
    "plt.yticks([60,80],labels=[])\n",
    "plt.ylim(45,99)\n",
    "plt.tight_layout()\n",
    "plt.savefig('acc_NO_D&S.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanACCS = np.array(accS).mean(axis=0) \n",
    "semACCS = np.array(accS).std(axis=0)/np.sqrt(len(accS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanACCF = np.array(accF).mean(axis=0) \n",
    "semACCF = np.array(accF).std(axis=0)/np.sqrt(len(accS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(4)+1,meanACCF,color='gray')\n",
    "plt.fill_between(np.arange(4)+1,np.array(meanACCF)-np.array(semACCF), \n",
    "                                 np.array(meanACCF)+np.array(semACCF), alpha=0.2, color = 'grey')\n",
    "plt.plot(np.arange(4)+1,meanACCS,color='k')\n",
    "plt.fill_between(np.arange(4)+1,np.array(meanACCS)-np.array(semACCS), \n",
    "                                 np.array(meanACCS)+np.array(semACCS), alpha=0.2, color = 'k')\n",
    "\n",
    "# plt.ylabel('accuracy (%)')\n",
    "plt.xlabel('difficulty')\n",
    "plt.legend(['fast','slow'],loc='best', shadow=True,fontsize=18)\n",
    "plt.ylim(45,99)\n",
    "plt.yticks([60,80],labels=[])\n",
    "plt.tight_layout()\n",
    "plt.savefig('acc_fast&slow_diff.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanACCQ = np.array(accQ).mean(axis=0) \n",
    "semACCQ = np.array(accQ).std(axis=0)/np.sqrt(len(accQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1,2,3,4],meanACCQ,color='k')\n",
    "plt.fill_between([1,2,3,4],np.array(meanACCQ)-np.array(semACCQ), \n",
    "                                 np.array(meanACCQ)+np.array(semACCQ), alpha=0.2, color = 'k')\n",
    "plt.ylabel('accuracy (%)')\n",
    "plt.xlabel('RT')\n",
    "plt.ylim(45,99)\n",
    "plt.tight_layout()\n",
    "plt.xticks([1,2,3,4],labels=['q1','q2','q3','q4'])\n",
    "plt.savefig('acc_RTquartiles.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'NOdetermistic':{'mean':list(meanACC_DNO),'sem':list(semACC_DNO)},\\\n",
    "         'NOstochastic':{'mean':list(meanACC_SNO),'sem':list(semACC_SNO)},\\\n",
    "        'accFast':{'mean':list(meanACCF),'sem':list(semACCF)},\\\n",
    "        'accSlow':{'mean':list(meanACCS),'sem':list(semACCS)},\n",
    "        'accRTquartile':{'mean':list(meanACCQ),'sem':list(semACCQ)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT RUN AGAIN !!!\n",
    "\n",
    "# write the result in file\n",
    "filename=path_results+'confidence_extra_analysis.json'\n",
    "# Serializing json  \n",
    "json_object_ = json.dumps(dict_) \n",
    "\n",
    "# Writing to sample.json \n",
    "with open(filename, \"w\") as outfile: \n",
    "    outfile.write(json_object_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-color",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-dallas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_no[part]['quartile'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no[part]['quartile'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no[part].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(accQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no[part]['discrimination_is_correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_no[3062])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no[3062] = df_no[3062].sort_values('v2') # Ordenas por el valor\n",
    "d['idx'] = np.arange(d.shape[0]) # le ponés un índice a cada elemento\n",
    "d['idx'] = d['idx'] / (d['idx'].max() + 1) * 100 # llevas esos índices al intervalo 0 ... 100\n",
    "d['quartile'] = d['idx'] // 25 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Ddf[3062])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no[3062].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100_000\n",
    "by = 18_000\n",
    "df = pd.DataFrame(np.random.uniform(0, 1, (n, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df.groupby(df.index // by).apply(pd.DataFrame.quantile, .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df.index // by).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-peace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
